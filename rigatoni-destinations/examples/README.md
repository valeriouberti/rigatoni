# Rigatoni Destinations Examples

This directory contains examples demonstrating different destination configurations for Rigatoni, with a focus on S3 data lake patterns.

## Quick Reference

| Example | Difficulty | Setup Time | Key Features | Best For |
|---------|-----------|------------|--------------|----------|
| [s3_basic.rs](#s3_basicrs) | ‚≠ê Beginner | < 5 min | JSON, no compression | First-time S3 users, simple setup |
| [s3_with_compression.rs](#s3_with_compressionrs) | ‚≠ê‚≠ê Intermediate | < 5 min | Gzip/Zstd compression | Cost optimization, large volumes |
| [s3_advanced.rs](#s3_advancedrs) | ‚≠ê‚≠ê‚≠ê Advanced | < 10 min | Parquet, partitioning, analytics | Production data lakes, analytics |

---

## s3_basic.rs

**The simplest S3 destination setup** - Perfect for getting started with S3 integration.

### What It Does

This example demonstrates:
- Basic S3 configuration with minimal settings
- JSON serialization (human-readable format)
- No compression (fastest for small volumes)
- Default key generation strategy
- Writing and flushing batches

### Prerequisites

Choose one of these setups:

#### Option 1: LocalStack (Recommended for Testing)

```bash
# Start LocalStack
docker compose -f docker/docker-compose.localstack.yml up -d

# Create test bucket
awslocal s3 mb s3://rigatoni-test-bucket

# Verify bucket exists
awslocal s3 ls
```

Set environment for LocalStack:
```bash
export AWS_ENDPOINT_URL=http://localhost:4566
export AWS_REGION=us-east-1
export AWS_ACCESS_KEY_ID=test
export AWS_SECRET_ACCESS_KEY=test
```

#### Option 2: Real AWS S3

Ensure AWS credentials are configured:
```bash
# Via AWS CLI
aws configure

# Or via environment
export AWS_ACCESS_KEY_ID="your-key-id"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_REGION="us-west-2"

# Create bucket (if needed)
aws s3 mb s3://your-bucket-name
```

### Running the Example

```bash
# With LocalStack (default)
cargo run -p rigatoni-destinations --example s3_basic --features s3,json

# With custom bucket
S3_BUCKET=my-bucket cargo run -p rigatoni-destinations --example s3_basic --features s3,json
```

### What You'll See

```
=== Basic S3 Destination Example ===

Using S3 bucket: rigatoni-test-bucket

Configuration:
  Bucket: rigatoni-test-bucket
  Region: us-east-1
  Prefix: Some("rigatoni/examples/basic")
  Format: Json
  Compression: None

Destination Metadata:
  Name: S3
  Type: s3
  Supports Transactions: false
  Supports Concurrent Writes: true

Creating sample change events...

Writing 4 events to S3...
Flushing to S3...

‚úì Successfully wrote events to S3!

You can now check your S3 bucket for the uploaded files.
Files will be organized by collection and partitioned by date/hour.

‚úì Destination closed successfully!
```

### Verifying the Output

#### With LocalStack:

```bash
# List uploaded files
awslocal s3 ls s3://rigatoni-test-bucket/rigatoni/examples/basic/ --recursive

# Download a file
awslocal s3 cp s3://rigatoni-test-bucket/rigatoni/examples/basic/... local-file.json

# View contents
cat local-file.json | jq .
```

#### With Real AWS:

```bash
# List uploaded files
aws s3 ls s3://your-bucket/rigatoni/examples/basic/ --recursive

# Download a file
aws s3 cp s3://your-bucket/rigatoni/examples/basic/... local-file.json

# View contents
cat local-file.json | jq .
```

### File Organization

Files are organized with this default structure:

```
s3://bucket/prefix/collection/YYYY/MM/DD/HH/batch-{uuid}.json
```

Example:
```
rigatoni-test-bucket/
‚îî‚îÄ‚îÄ rigatoni/examples/basic/
    ‚îú‚îÄ‚îÄ users/
    ‚îÇ   ‚îî‚îÄ‚îÄ 2025/01/21/14/
    ‚îÇ       ‚îú‚îÄ‚îÄ batch-abc123.json
    ‚îÇ       ‚îî‚îÄ‚îÄ batch-def456.json
    ‚îî‚îÄ‚îÄ products/
        ‚îî‚îÄ‚îÄ 2025/01/21/14/
            ‚îî‚îÄ‚îÄ batch-xyz789.json
```

### Key Concepts

#### S3Config Builder

The configuration uses a builder pattern:

```rust
let config = S3Config::builder()
    .bucket("rigatoni-test-bucket")  // Required: S3 bucket name
    .region("us-east-1")              // Required: AWS region
    .prefix("rigatoni/examples/basic") // Optional: Key prefix
    .build()?;
```

Additional options (shown with defaults):
```rust
.format(SerializationFormat::Json)     // Default format
.compression(Compression::None)         // No compression
.max_retries(3)                        // Retry failed uploads
.timeout(Duration::from_secs(30))      // Upload timeout
```

#### Destination Trait

The example shows the `Destination` trait in action:

```rust
// Write a batch of events
destination.write_batch(&events).await?;

// Flush buffered data to S3
destination.flush().await?;

// Clean shutdown
destination.close().await?;
```

#### SerializationFormat::Json

JSON format characteristics:
- **Pros:** Human-readable, widely supported, easy debugging
- **Cons:** Larger file size, slower parsing than binary formats
- **Best for:** Development, debugging, small-medium volumes

### Modifying the Example

#### Change Bucket and Region

```rust
let config = S3Config::builder()
    .bucket("my-production-bucket")
    .region("eu-west-1")
    .prefix("mongodb/cdc")
    .build()?;
```

#### Use Different Prefix Structure

```rust
// No prefix (root of bucket)
.prefix("")

// Deeper hierarchy
.prefix("data-lake/raw/mongodb/cdc")

// Include environment
.prefix(format!("mongodb/{}/cdc", env::var("ENV")?))
```

#### Add Custom Endpoint (for LocalStack)

```rust
let config = S3Config::builder()
    .bucket("rigatoni-test-bucket")
    .region("us-east-1")
    .endpoint("http://localhost:4566")  // LocalStack endpoint
    .build()?;
```

### Common Issues

#### Error: "Bucket does not exist"

**Solution:**
```bash
# LocalStack
awslocal s3 mb s3://rigatoni-test-bucket

# Real AWS
aws s3 mb s3://your-bucket-name --region us-east-1
```

#### Error: "Access Denied"

**Checklist:**
1. Are AWS credentials configured? `aws configure list`
2. Does IAM user/role have S3 write permissions?
3. Is bucket policy allowing writes?

**Minimum IAM policy:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name/*",
        "arn:aws:s3:::your-bucket-name"
      ]
    }
  ]
}
```

#### Error: "Connection refused" (LocalStack)

**Check LocalStack is running:**
```bash
docker compose -f docker/docker-compose.localstack.yml ps

# Restart if needed
docker compose -f docker/docker-compose.localstack.yml restart
```

#### Files not appearing in S3

**Checklist:**
1. Did you call `flush()`? Files are buffered until flush
2. Check the correct bucket and prefix
3. List with `--recursive` flag to see nested keys

### Production Considerations

Before using in production:

- [ ] **Add compression** - See [s3_with_compression.rs](#s3_with_compressionrs)
- [ ] **Consider binary format** - Parquet for analytics workloads
- [ ] **Enable S3 lifecycle policies** - Archive old data to Glacier
- [ ] **Set up CloudWatch alarms** - Monitor upload failures
- [ ] **Use VPC endpoints** - Reduce data transfer costs
- [ ] **Enable versioning** - Protect against accidental deletion
- [ ] **Configure server-side encryption** - Encrypt data at rest

### Next Steps

After mastering basic S3 usage:
1. Try [s3_with_compression.rs](#s3_with_compressionrs) to reduce storage costs
2. Try [s3_advanced.rs](#s3_advancedrs) for production data lake patterns

---

## s3_with_compression.rs

**Optimize storage costs and bandwidth** with compression.

### What It Does

This example demonstrates:
- Gzip compression (industry standard, wide compatibility)
- Zstandard compression (better ratio, faster than gzip)
- Compression trade-offs and benchmarking
- Storage cost optimization

### Prerequisites

Same as [s3_basic.rs](#prerequisites) - LocalStack or real AWS S3.

### Running the Example

#### With Gzip Compression:

```bash
cargo run -p rigatoni-destinations --example s3_with_compression \
  --features s3,json,gzip
```

#### With Zstandard Compression:

```bash
cargo run -p rigatoni-destinations --example s3_with_compression \
  --features s3,json,zstandard
```

#### Without Compression (for comparison):

```bash
cargo run -p rigatoni-destinations --example s3_with_compression \
  --features s3,json
```

### What You'll See

```
=== S3 Destination with Compression Example ===

Configuration:
  Bucket: rigatoni-test-bucket
  Compression: Zstd

Creating 100 sample events...
Events created: 100

Writing to S3 with Zstd compression...

‚úì Successfully wrote 100 events
  Time taken: 542ms

Compression benefits:
  - Reduced storage costs
  - Lower bandwidth usage
  - Faster uploads for large batches

üìä Zstd: ~75-85% compression ratio (faster than gzip!)

‚úì Example completed!
```

### Compression Comparison

| Algorithm | Ratio | Speed | CPU Usage | Best For |
|-----------|-------|-------|-----------|----------|
| **None** | 0% | Fastest | Minimal | Small volumes, low latency |
| **Gzip** | 70-80% | Medium | Medium | Industry standard, wide compatibility |
| **Zstd** | 75-85% | Fast | Low-Medium | Modern systems, best balance |

### Key Concepts

#### Enabling Compression

```rust
use rigatoni_destinations::s3::Compression;

let config = S3Config::builder()
    .bucket("my-bucket")
    .region("us-east-1")
    .compression(Compression::Gzip)  // or Compression::Zstd
    .build()?;
```

Available options:
```rust
Compression::None    // No compression (default)
Compression::Gzip    // Requires "gzip" feature
Compression::Zstd    // Requires "zstandard" feature
```

#### Storage Cost Savings

Example savings for 100GB/month of MongoDB change events:

**Without compression:**
- S3 Standard: $2.30/month
- Data transfer: $0.90/month
- **Total: $3.20/month**

**With Gzip (75% reduction):**
- S3 Standard: $0.58/month
- Data transfer: $0.23/month
- **Total: $0.81/month**
- **Savings: $2.39/month (75%)**

At scale (10TB/month): **Save $239/month!**

#### File Extensions

Compressed files automatically get correct extensions:

```
# Gzip
batch-abc123.json.gz

# Zstandard
batch-def456.json.zst
```

This enables automatic decompression in most analytics tools.

### Verifying Compression

#### Check File Sizes:

```bash
# List files with sizes
awslocal s3 ls s3://rigatoni-test-bucket/rigatoni/examples/compressed/ \
  --recursive --human-readable

# Compare compressed vs uncompressed
awslocal s3 ls s3://rigatoni-test-bucket/rigatoni/examples/basic/ \
  --recursive --human-readable --summarize
awslocal s3 ls s3://rigatoni-test-bucket/rigatoni/examples/compressed/ \
  --recursive --human-readable --summarize
```

#### Decompress and View:

```bash
# Download compressed file
awslocal s3 cp s3://bucket/path/batch.json.gz - | gunzip | jq .

# For Zstandard
awslocal s3 cp s3://bucket/path/batch.json.zst - | zstd -d | jq .
```

### When to Use Compression

**Use Gzip when:**
- Analytics tools require gzip (Athena, Redshift Spectrum)
- Need maximum compatibility
- Storage cost is primary concern

**Use Zstandard when:**
- Performance matters (faster compression/decompression)
- Modern analytics stack (Presto, Spark support zstd)
- Want best compression ratio with good speed

**Skip compression when:**
- Very small batches (< 1KB)
- CPU constrained environment
- Need absolute minimum latency
- Already using compressed format (Parquet has internal compression)

### Modifying the Example

#### Combine with Parquet

```rust
let config = S3Config::builder()
    .bucket("my-bucket")
    .format(SerializationFormat::Parquet)
    .compression(Compression::Zstd)  // Double compression!
    .build()?;
```

Note: Parquet has internal compression, so external compression provides limited benefit.

#### Adjust Batch Size for Better Compression

```rust
// Larger batches compress better
let config = PipelineConfig::builder()
    .batch_size(10000)  // Increased from default
    .batch_timeout(Duration::from_secs(30))
    .build()?;
```

Larger batches = better compression ratios due to more repetitive data.

### Common Issues

#### Error: "Unsupported compression"

**Solution:** Enable the required feature flag:
```bash
# For Gzip
cargo run --example s3_with_compression --features gzip

# For Zstandard
cargo run --example s3_with_compression --features zstandard
```

#### Analytics tools can't read compressed files

**Most tools auto-detect compression from extension:**
- AWS Athena: Auto-detects `.gz` and `.zst`
- Presto/Trino: Auto-detects both
- Spark: Requires codec configuration for Zstd

**If issues occur, verify file extension is correct:**
```bash
awslocal s3 ls s3://bucket/path/ --recursive
# Should see .json.gz or .json.zst
```

### Performance Benchmarks

Results from 10,000 events (~50MB uncompressed):

| Format | Size | Upload Time | Ratio | Throughput |
|--------|------|-------------|-------|------------|
| JSON (uncompressed) | 50MB | 2.1s | - | 23.8 MB/s |
| JSON + Gzip | 12MB | 1.8s | 76% | 27.8 MB/s |
| JSON + Zstd | 10MB | 1.5s | 80% | 33.3 MB/s |

**Key insight:** Compression actually *improves* upload speed due to reduced network I/O!

### Next Steps

After mastering compression:
1. Try [s3_advanced.rs](#s3_advancedrs) for partitioning and analytics
2. Read [S3 Configuration Guide](../../docs/guides/s3-configuration.md)

---

## s3_advanced.rs

**Production-ready data lake patterns** with partitioning and multiple formats.

### What It Does

This example demonstrates:
- Multiple serialization formats (JSON, CSV, Parquet, Avro)
- Hive-style partitioning for analytics
- Advanced compression strategies
- Production-grade retry configuration
- Data lake best practices

### Prerequisites

Same as previous examples - LocalStack or real AWS S3.

### Running the Example

#### With All Features:

```bash
cargo run -p rigatoni-destinations --example s3_advanced --all-features
```

#### With Specific Format:

```bash
# CSV format
cargo run -p rigatoni-destinations --example s3_advanced \
  --features s3,csv,gzip

# Parquet format (recommended for analytics)
cargo run -p rigatoni-destinations --example s3_advanced \
  --features s3,parquet,zstandard

# Avro format
cargo run -p rigatoni-destinations --example s3_advanced \
  --features s3,avro
```

### What You'll See

```
=== Advanced S3 Destination Example ===

Configuration:
  Bucket: rigatoni-test-bucket
  Format: Parquet
  Compression: Zstd
  Partitioning: Hive-style (for analytics)

üìÅ Key Generation Strategy:
  Pattern: collection={collection}/year={year}/month={month}/day={day}/

  Benefits:
  ‚úì Automatic partition discovery in Athena/Presto/Spark
  ‚úì Efficient time-range queries
  ‚úì Organized data lake structure

Creating sample events across collections...
  Total events: 30
  Collections: users, orders, products

Writing to S3...

‚úì Successfully wrote events!
  Time taken: 1.2s

üìä Data Organization:
  Each collection will have separate partitioned files:
  - analytics/mongodb-cdc/collection=users/year=2025/month=01/day=21/...
  - analytics/mongodb-cdc/collection=orders/year=2025/month=01/day=21/...
  - analytics/mongodb-cdc/collection=products/year=2025/month=01/day=21/...

üí° Analytics Usage:
  You can now query this data using:
  - AWS Athena: CREATE EXTERNAL TABLE with PARTITIONED BY
  - Presto/Trino: Automatic partition discovery
  - Apache Spark: Read with partition pruning

‚úì Example completed!
```

### Key Concepts

#### Hive-Style Partitioning

Partitioning organizes data for efficient analytics queries:

```rust
use rigatoni_destinations::s3::KeyGenerationStrategy;

let config = S3Config::builder()
    .bucket("my-bucket")
    .key_strategy(KeyGenerationStrategy::HivePartitioned)
    .build()?;
```

**File structure:**
```
s3://bucket/prefix/
  collection=users/
    year=2025/
      month=01/
        day=21/
          hour=14/
            batch-abc123.parquet
  collection=orders/
    year=2025/
      month=01/
        day=21/
          hour=14/
            batch-def456.parquet
```

**Benefits:**
- **Partition pruning:** Query only relevant partitions
- **Auto-discovery:** Athena/Presto automatically detect partitions
- **Cost savings:** Scan less data = lower costs
- **Performance:** 10-100x faster queries with proper partitioning

#### SerializationFormat Options

```rust
use rigatoni_destinations::s3::SerializationFormat;

// JSON - Human-readable, debugging
.format(SerializationFormat::Json)

// CSV - Simple, widely compatible
.format(SerializationFormat::Csv)

// Parquet - Columnar, best for analytics (recommended!)
.format(SerializationFormat::Parquet)

// Avro - Row-based, schema evolution
.format(SerializationFormat::Avro)
```

**Format Comparison:**

| Format | Type | Size | Query Speed | Schema | Best For |
|--------|------|------|-------------|--------|----------|
| JSON | Row | 1x | Slow | Flexible | Development, debugging |
| CSV | Row | 0.5x | Slow | Fixed | Simple exports, spreadsheets |
| Parquet | Columnar | 0.1x | **Very Fast** | Strict | Analytics, data lakes |
| Avro | Row | 0.3x | Medium | Evolvable | Streaming, schema changes |

**Recommendation:** Use **Parquet** for production data lakes!

#### KeyGenerationStrategy Options

```rust
// Hive-style partitioning (recommended for analytics)
.key_strategy(KeyGenerationStrategy::HivePartitioned)
// Example: collection=users/year=2025/month=01/day=21/batch.parquet

// Date-based partitioning (simpler)
.key_strategy(KeyGenerationStrategy::DateBased)
// Example: users/2025/01/21/batch.parquet

// UUID-based (no partitioning)
.key_strategy(KeyGenerationStrategy::Uuid)
// Example: users/abc-123-def-456.parquet

// Custom strategy
.key_strategy(KeyGenerationStrategy::Custom(Box::new(my_fn)))
```

#### Production Retry Configuration

```rust
let config = S3Config::builder()
    .bucket("my-bucket")
    .max_retries(5)                              // More retries
    .timeout(Duration::from_secs(60))            // Longer timeout
    .retry_delay(Duration::from_secs(2))         // Delay between retries
    .build()?;
```

### Analytics Integration

#### AWS Athena

Create external table:

```sql
CREATE EXTERNAL TABLE mongodb_users (
  _id INT,
  name STRING,
  event_type STRING,
  status STRING,
  created_at TIMESTAMP,
  metadata STRUCT<version:STRING, source:STRING, environment:STRING>
)
PARTITIONED BY (
  collection STRING,
  year INT,
  month INT,
  day INT
)
STORED AS PARQUET
LOCATION 's3://rigatoni-test-bucket/analytics/mongodb-cdc/';

-- Discover partitions
MSCK REPAIR TABLE mongodb_users;

-- Query specific partition (fast!)
SELECT * FROM mongodb_users
WHERE collection = 'users'
  AND year = 2025
  AND month = 1
  AND day = 21;
```

#### Apache Spark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MongoDB CDC").getOrCreate()

# Read partitioned Parquet
df = spark.read.parquet("s3://rigatoni-test-bucket/analytics/mongodb-cdc/")

# Automatic partition pruning
users_today = df.filter(
    (df.collection == "users") &
    (df.year == 2025) &
    (df.month == 1) &
    (df.day == 21)
)

users_today.show()
```

#### Presto/Trino

```sql
-- Auto-discovers partitions
SELECT collection, COUNT(*)
FROM hive.default.mongodb_cdc
WHERE year = 2025 AND month = 1
GROUP BY collection;
```

### Modifying the Example

#### Custom Partitioning Strategy

```rust
fn custom_key_fn(
    collection: &str,
    timestamp: &DateTime<Utc>,
    batch_id: &str,
) -> String {
    format!(
        "env=production/source=mongodb/collection={}/date={}/hour={}/{}.parquet",
        collection,
        timestamp.format("%Y-%m-%d"),
        timestamp.format("%H"),
        batch_id
    )
}

let config = S3Config::builder()
    .bucket("my-bucket")
    .key_strategy(KeyGenerationStrategy::Custom(Box::new(custom_key_fn)))
    .build()?;
```

#### Combine Best Practices

```rust
let config = S3Config::builder()
    .bucket("production-data-lake")
    .region("us-west-2")
    .prefix("raw/mongodb-cdc")
    .format(SerializationFormat::Parquet)     // Best for analytics
    .compression(Compression::Zstd)            // Best compression
    .key_strategy(KeyGenerationStrategy::HivePartitioned)  // Best partitioning
    .max_retries(5)                            // Production retries
    .timeout(Duration::from_secs(120))         // Longer timeout
    .build()?;
```

### Common Issues

#### Athena: "HIVE_PARTITION_SCHEMA_MISMATCH"

**Cause:** Partition column types don't match table definition

**Solution:** Ensure partition columns are defined correctly:
```sql
PARTITIONED BY (
  collection STRING,  -- Not INT!
  year INT,           -- Not STRING!
  month INT,
  day INT
)
```

#### Spark: "Partition discovery too slow"

**Solution:** Explicitly specify partition schema:
```python
df = spark.read \
    .option("basePath", "s3://bucket/prefix/") \
    .parquet("s3://bucket/prefix/collection=users/")
```

#### Parquet files too small

**Cause:** Small batch sizes create many tiny files

**Solution:** Increase batch size:
```rust
let config = PipelineConfig::builder()
    .batch_size(10000)  // Larger batches = larger Parquet files
    .build()?;
```

**Ideal Parquet file size:** 128MB - 1GB per file

### Production Checklist

Before deploying to production data lake:

- [ ] **Use Parquet format** - Best performance and compression
- [ ] **Enable Hive partitioning** - Required for efficient queries
- [ ] **Set appropriate batch size** - 10,000+ events for optimal file sizes
- [ ] **Enable compression** - Zstd for Parquet
- [ ] **Configure S3 lifecycle** - Archive old partitions to Glacier
- [ ] **Set up AWS Glue Crawler** - Auto-discover partitions
- [ ] **Create Athena views** - Simplify queries for analysts
- [ ] **Monitor upload failures** - CloudWatch alarms
- [ ] **Test partition pruning** - Verify queries are efficient
- [ ] **Document schema** - Maintain schema registry

### Performance Tuning

#### Optimize for Query Performance

```rust
// Large batch size = larger Parquet files = better analytics performance
.batch_size(50000)
.batch_timeout(Duration::from_secs(300))  // 5 minutes
```

#### Optimize for Write Throughput

```rust
// Smaller batches = lower latency, more frequent writes
.batch_size(1000)
.batch_timeout(Duration::from_secs(10))
```

#### Balance Both

```rust
// Medium batch size for balanced performance
.batch_size(10000)
.batch_timeout(Duration::from_secs(60))
```

### Cost Optimization

**Strategies to reduce costs:**

1. **Compression:** Zstd reduces storage by 75-85%
2. **Partitioning:** Query only necessary partitions
3. **Lifecycle policies:** Archive old data to Glacier
4. **VPC endpoints:** Eliminate data transfer costs
5. **Intelligent tiering:** Auto-move infrequent data

**Example savings for 1TB/month:**
- Without optimization: $23/month
- With Parquet + Zstd + lifecycle: $2/month
- **Savings: 91%!**

### Next Steps

After mastering advanced S3 patterns:
1. Read [S3 Configuration Guide](../../docs/guides/s3-configuration.md)
2. Review [Production Deployment Guide](../../docs/guides/production-deployment.md)
3. Explore [Metrics Example](../../rigatoni-core/examples/README.md#metrics_prometheusrs)

---

## Additional Resources

- [Main Examples README](../../examples/README.md) - Complete examples guide
- [Core Examples](../../rigatoni-core/examples/README.md) - Pipeline and metrics examples
- [S3 Configuration Guide](../../docs/guides/s3-configuration.md) - Detailed S3 setup
- [Architecture Guide](../../docs/architecture.md) - System design
- [API Documentation](https://docs.rs/rigatoni-destinations) - API reference

---

## Feature Flags Reference

When running examples, you'll need these feature flags:

| Feature | Description | Required For |
|---------|-------------|--------------|
| `s3` | S3 destination | All S3 examples |
| `json` | JSON serialization | JSON format |
| `csv` | CSV serialization | CSV format |
| `parquet` | Parquet serialization | Parquet format (analytics) |
| `avro` | Avro serialization | Avro format |
| `gzip` | Gzip compression | Compressed uploads |
| `zstandard` | Zstandard compression | Better compression |

**Common combinations:**

```bash
# Development
--features s3,json

# Production data lake
--features s3,parquet,zstandard

# All features
--all-features
```

---

## Environment Variables

Examples support these environment variables:

```bash
# S3 bucket name (defaults to rigatoni-test-bucket)
export S3_BUCKET="your-bucket-name"

# AWS region (defaults to us-east-1)
export AWS_REGION="us-west-2"

# LocalStack endpoint (for local testing)
export AWS_ENDPOINT_URL="http://localhost:4566"

# AWS credentials
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
```

---

## Contributing Examples

When adding new destination examples:

1. **Follow naming convention** - `{destination}_{feature}.rs`
2. **Include comprehensive docs** - Prerequisites, features, use cases
3. **Support both LocalStack and real AWS** - Test locally first
4. **Show best practices** - Production-ready configurations
5. **Add to this README** - Document thoroughly
6. **Test from clean state** - Verify setup instructions work

---

## Need Help?

- **Issues:** [GitHub Issues](https://github.com/valeriouberti/rigatoni/issues)
- **Discussions:** [GitHub Discussions](https://github.com/valeriouberti/rigatoni/discussions)
- **Guides:** [Documentation](https://valeriouberti.github.io/rigatoni/)
